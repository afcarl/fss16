
# Project: csc591

## Project Work

The premises of this subject are that:

- very small models can do (nearly) as well as more complex ones;
- models change, a lot:
     - over time, as new data arrives
     - over different sub-samples since different parts of the data support different conclusions

<img align=left size=400 src="https://storage.googleapis.com/vidsums/6772ff56-2fc3-4125-8d59-f115533708f1_text.gif">

So, go on make me a liar. Prove me wrong. From any source of SE data, show that:

- Models learned from data at time _i_ **are not** very different at some later time _j_;
- Models learned from different sub-samples of the data **are not**  very different
- The performance of small, carefully built, models are **much worse** (*) that more complex ones.

(*) where "worse" means allowing for variances over
different sub-samples of the training data and
includes tests for statistical significance and
effect size.

